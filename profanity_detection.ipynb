{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "source": [
    "import re\r\n",
    "import string\r\n",
    "import nltk\r\n",
    "nltk.download('stopwords')\r\n",
    "nltk.download('punkt')\r\n",
    "from nltk.corpus import stopwords\r\n",
    "from nltk.stem import PorterStemmer\r\n",
    "from nltk.tokenize import word_tokenize\r\n",
    "import numpy as np\r\n",
    "from nltk.stem import WordNetLemmatizer\r\n",
    "from profanity_check import predict, predict_prob\r\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\r\n",
    "import csv"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\binta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\binta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "source": [
    "def cleaning_url(raw_url)->str:\r\n",
    "    url_ = re.sub(';|,| |-|/|:|\\.',' ',raw_url)\r\n",
    "    # url_ = re.sub('[;,-/:\\.]',' ',url[0])\r\n",
    "    url_ = url_.lower()\r\n",
    "    url_ = re.sub(r'https|com|net|co|us|fr|uk|id|care|http|www', ' ', url_)\r\n",
    "    return url_\r\n",
    "#aman"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "source": [
    "# #test url cleaning\r\n",
    "# text = \"https://stackoverflow.com/questions/55407843/how-to-handle-url-links-in-text-data-while-preprocessing-data-in-nlp\"\r\n",
    "# cleaning_url(text)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "source": [
    "#test stem vs lemmatize\r\n",
    "# stemmer = PorterStemmer()\r\n",
    "# lemmatizer = WordNetLemmatizer()\r\n",
    "# def stem_vs_lemmatize(text):\r\n",
    "#     print(stemmer.stem(text))\r\n",
    "#     print(lemmatizer.lemmatize(text))\r\n",
    "# stem_vs_lemmatize('cry')\r\n",
    "# stem_vs_lemmatize('handler')\r\n",
    "# stem_vs_lemmatize(\"dont\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "source": [
    "stopwords_english = stopwords.words('english')\r\n",
    "stemmer = PorterStemmer()\r\n",
    "# lemmatizer = WordNetLemmatizer()\r\n",
    "\r\n",
    "#its yes no memukul -> pukul,  4 -> 100\r\n",
    "\r\n",
    "mtokenize = ToktokTokenizer()\r\n",
    "\r\n",
    "# stopword tambahan\r\n",
    "more_stopword = set([])\r\n",
    "\r\n",
    "# Happy Emoticons\r\n",
    "emoticons_happy = set([\r\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\r\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\r\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\r\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\r\n",
    "    '<3'\r\n",
    "])\r\n",
    "\r\n",
    "# Sad Emoticons\r\n",
    "emoticons_sad = set([\r\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\r\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\r\n",
    "    ':c', ':{', '>:\\\\', ';('\r\n",
    "])\r\n",
    "\r\n",
    "# all emoticons (happy + sad)\r\n",
    "emoticons = emoticons_happy.union(emoticons_sad)\r\n",
    "\r\n",
    "def cleaning_text(raw_text)->str:\r\n",
    "    '''\r\n",
    "    :rtype: string\r\n",
    "    :type text : string\r\n",
    "    :param text: type string, text mentah\r\n",
    "    :return: type string, text yang sudah dibersihkan\r\n",
    "    '''\r\n",
    "    # print(\"-----------------------------------------------------------------\")\r\n",
    "    text_ = re.sub(r'\\n', ' ', raw_text)  # menghapus baris baru\r\n",
    "    # print(\"sebelum : \"+text_)\r\n",
    "    text_ = text_.strip(\" \")  # menghapus karakter spasi pada awal dan akhir kalimat\r\n",
    "    # print(text_)\r\n",
    "    symbols_regrex_pattern = re.compile(pattern=\"[\"\r\n",
    "                                                u\"\\U0001F600-\\U0001F64F\"  # emoticons\r\n",
    "                                                u\"\\U0001F300-\\U0001F5FF\"  # simbol & piktograf\r\n",
    "                                                u\"\\U0001F680-\\U0001F6FF\"  # simbol transport & map\r\n",
    "                                                \"]+\", flags=re.UNICODE)    \r\n",
    "    text_ = symbols_regrex_pattern.sub(r' ', text_)  \r\n",
    "    # print(text_)\r\n",
    "    text_ = text_.lower() \r\n",
    "    # print(text_)\r\n",
    "    text_ = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', ' ', text_, flags=re.MULTILINE)  # menghapus hyperlink\r\n",
    "    # print(text_)\r\n",
    "    text_ = re.sub('[0-9]+', ' ', text_)  # menghapus angka\r\n",
    "    # print(text_)\r\n",
    "    tokens = word_tokenize(text_)  # tokenize kata-kata\r\n",
    "    tokens = mtokenize.tokenize(text_)  # tokenize kata-kata\r\n",
    "    # print(tokens)\r\n",
    "\r\n",
    "    text_clean = []\r\n",
    "\r\n",
    "    for word in tokens:\r\n",
    "        # print(word)\r\n",
    "        if (word not in stopwords_english and  # menghilangkan stopwords. proses filtering\r\n",
    "                word not in emoticons and  # menghilangkan emoticons. proses filtering\r\n",
    "                word not in more_stopword and\r\n",
    "                word not in string.punctuation):  # menghilangkan tanda baca. proses filtering\r\n",
    "            # stem_word = lemmatizer.lemmatize(word)\r\n",
    "            # print(word)\r\n",
    "            stem_word = stemmer.stem(word)  # mengubah ke kata dasar. proses stemming\r\n",
    "            # print(stem_word)\r\n",
    "            text_clean.append(stem_word)\r\n",
    "\r\n",
    "    text_clean = \" \".join(text_clean)\r\n",
    "    # print(text_clean)\r\n",
    "    text_clean = re.sub(r'[?\\|$\\.!_:\")(\\+,\\*&%#@`\\'\\\"=\\\\/]', ' ', text_clean)  # menghapus beberapa karakter khusus\r\n",
    "    # print(text_clean)\r\n",
    "    text_clean = re.sub(' +', ' ', text_clean)  # lebih dari satu spasi yang berdekatan, menjadi satu spasi\r\n",
    "    text_ = text_.strip(\" \")  # menghapus karakter spasi pada awal dan akhir kalimat\r\n",
    "\r\n",
    "    # print(\"sesudah : \", text_clean)\r\n",
    "    # print(\"-----------------------------------------------------------------\")\r\n",
    "    return text_clean\r\n",
    "#aman\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "source": [
    "#test url cleaning\r\n",
    "# text = \"https://stackoverflow.com/questions/55407843/how-to-handle-url-links-in-text-data-while-preprocessing-data-in-nlp\"\r\n",
    "# text = 'python - \"UnboundLocalError: local variable referenced before assignment\" after an if statement - Stack Overflow'\r\n",
    "# text = \"\"\"I have also tried searching for the answer but I don't understand the answers to other people's similar problems...\r\n",
    "# tfile= open(\"/home/path/to/file\",'r') \r\n",
    "# def temp_sky(lreq, breq):\r\n",
    "#     for line in \"\"\"\r\n",
    "# cleaning_text(text)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "source": [
    "forbidden_word = []\r\n",
    "with open('data/bad-bad-word-cleaned.csv', newline='') as f:\r\n",
    "    reader = csv.reader(f)\r\n",
    "    data = list(reader)\r\n",
    "for i in data:\r\n",
    "    forbidden_word.append(i[0])\r\n",
    "\r\n",
    "# print(forbidden_word)\r\n",
    "\r\n",
    "def check_forbidden_word(raw_text)->int:\r\n",
    "    '''\r\n",
    "    :rtype: integer\r\n",
    "    :type text : string\r\n",
    "    :param text: type string, text \r\n",
    "    :return: integer, 1 jika menemukan forbidden word, 0 jika tidak ditemukan forbidden word\r\n",
    "    '''    \r\n",
    "    text_ = raw_text.lower()\r\n",
    "    for fword in forbidden_word:\r\n",
    "        if fword in text_:\r\n",
    "            return 1\r\n",
    "    return 0\r\n",
    "#aman"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "source": [
    "# text = \"Pornhub is the world’s leading free porn site. Choose from millions of hardcore videos that stream quickly and in high quality, including amazing VR Porn. The largest adult site on the Internet just keeps getting better. We have more pornstars and real amateurs than anyone else. It’s fast, it’s free, it’s Pornhub!\"\r\n",
    "# text = \"PORN\"\r\n",
    "# check_forbidden_word(text)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "source": [
    "def check_ip_domain(raw_url)->int:\r\n",
    "    url_ = re.sub(r'https|http|/|:', ' ', raw_url)\r\n",
    "    url_ = re.sub(' +', ' ', url_)\r\n",
    "    url_ = url_.strip(\" \")\r\n",
    "    url_ = url_.split(\" \")\r\n",
    "    # print(url_[0].split('.'))\r\n",
    "    # print(url_[0].split('.')[-1])\r\n",
    "    if not url_[0].split('.')[-1].isalpha() :\r\n",
    "        return 1\r\n",
    "    else :\r\n",
    "        return 0\r\n",
    "#aman        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "source": [
    "# text = \"http://192.158.1.38/safely-collecting-hot-flows-from-android-native-ui-f22f645edb44\"\r\n",
    "# text = \"http://0xC0.0x00.0x02.0xEB/safely-collecting-hot-flows-from-android-native-ui-f22f645edb44\"\r\n",
    "# text = 'https://stackoverflow.com/questions/30626182/determine-if-host-is-domain-name-or-ip-in-python'\r\n",
    "# text = \"\"\r\n",
    "# check_ip_domain(text)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "source": [
    "def check_profanity_check_lib(text)->int:\r\n",
    "    # print(predict_prob([text]))\r\n",
    "    # print(predict([text]))\r\n",
    "    # if predict_prob([text])[0] > 0.05:\r\n",
    "    #     return 1\r\n",
    "    return predict([text])[0]\r\n",
    "#aman"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "source": [
    "# text = \"Pornhub is the world’s leading free porn site. Choose from millions of hardcore videos that stream quickly and in high quality, including amazing VR Porn. The largest adult site on the Internet just keeps getting better. We have more pornstars and real amateurs than anyone else. It’s fast, it’s free, it’s Pornhub!\"\r\n",
    "# text = \"python unboundlocalerror local variabl referenc assign statement stack overflow\"\r\n",
    "# text = \"Free Porn, Sex, Tube Videos, XXX Pics, Pussy in Porno Movies - XNXX.COM\"\r\n",
    "# check_profanity_check_lib(text)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "source": [
    "def profanity_check_rating(raw_rating):\r\n",
    "    rating_ = raw_rating.lower()\r\n",
    "    forbidden_rating = [\"mature\", \"adult\", \"restricted\",\"RTA-5042-1996-1400-1577-RTA\"]\r\n",
    "    forbidden_rating = [fr.lower() for fr in forbidden_rating]\r\n",
    "    for fr in forbidden_rating:\r\n",
    "        if fr in rating_:\r\n",
    "            return 1\r\n",
    "    return 0\r\n",
    "#aman"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "source": [
    "def matrix_profanity_check_rating(raw_rating):\r\n",
    "    rating_ = raw_rating.lower()\r\n",
    "    forbidden_rating = [\"mature\", \"adult\", \"restricted\",\"RTA-5042-1996-1400-1577-RTA\"]\r\n",
    "    forbidden_rating = [fr.lower() for fr in forbidden_rating]\r\n",
    "    for fr in forbidden_rating:\r\n",
    "        if fr in rating_:\r\n",
    "            return {\"rating\" : 1}\r\n",
    "\r\n",
    "    return {\"rating\" : 0}\r\n",
    "#aman"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "source": [
    "# test = \"RTA-5042-1996-1400-1577-RTA\"\r\n",
    "# profanity_check_rating(test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "source": [
    "def profanity_check_url(raw_url):\r\n",
    "    url_clean = cleaning_url(raw_url)\r\n",
    "    text_ = cleaning_text(url_clean)\r\n",
    "\r\n",
    "    v_check_forbidden_word = check_forbidden_word(url_clean)\r\n",
    "    if v_check_forbidden_word == 1 : return 1\r\n",
    "\r\n",
    "    v_check_profanity_check_lib = check_profanity_check_lib(text_)\r\n",
    "    if v_check_profanity_check_lib == 1 : return 1\r\n",
    "\r\n",
    "    v_check_ip_domain = check_ip_domain(raw_url)\r\n",
    "    if v_check_ip_domain == 1 : return 1\r\n",
    "    \r\n",
    "    return 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "source": [
    "def matrix_profanity_check_url(raw_url):\r\n",
    "    url_clean = cleaning_url(raw_url)\r\n",
    "    text_ = cleaning_text(url_clean)\r\n",
    "\r\n",
    "    v_check_forbidden_word = check_forbidden_word(url_clean)\r\n",
    "    v_check_profanity_check_lib = check_profanity_check_lib(text_)\r\n",
    "    v_check_ip_domain = check_ip_domain(raw_url)\r\n",
    "    \r\n",
    "    return {\"fword\" : v_check_forbidden_word,\"proflib\" : v_check_profanity_check_lib,\"ipdomain\" : v_check_ip_domain}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "source": [
    "def profanity_check_text(raw_text)->int:\r\n",
    "    text_ = cleaning_text(raw_text)\r\n",
    "\r\n",
    "    v_check_forbidden_word = check_forbidden_word(raw_text)\r\n",
    "    if v_check_forbidden_word == 1 : return 1\r\n",
    "\r\n",
    "    v_check_profanity_check_lib = check_profanity_check_lib(text_)\r\n",
    "    if v_check_profanity_check_lib == 1 : return 1\r\n",
    "\r\n",
    "    return 0\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "source": [
    "def matrix_profanity_check_text(raw_text)->int:\r\n",
    "    text_ = cleaning_text(raw_text)\r\n",
    "\r\n",
    "    v_check_forbidden_word = check_forbidden_word(raw_text)\r\n",
    "    v_check_profanity_check_lib = check_profanity_check_lib(text_)\r\n",
    "\r\n",
    "    return {\"fword\" : v_check_forbidden_word,\"proflib\" : v_check_profanity_check_lib}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "source": [
    "def profanity_check_overall(url, title, description, keyword, rating):\r\n",
    "    v_profanity_check_url = profanity_check_url(url)\r\n",
    "    if v_profanity_check_url == 1 : return 1\r\n",
    "\r\n",
    "    v_profanity_check_title = profanity_check_text(title)\r\n",
    "    if v_profanity_check_title == 1 : return 1\r\n",
    "\r\n",
    "    v_profanity_check_description = profanity_check_text(description)\r\n",
    "    if v_profanity_check_description == 1 : return 1\r\n",
    "\r\n",
    "    v_profanity_check_keyword = profanity_check_text(keyword)\r\n",
    "    if v_profanity_check_keyword == 1 : return 1\r\n",
    "\r\n",
    "    v_profanity_check_rating = profanity_check_rating(rating)\r\n",
    "    if v_profanity_check_rating == 1 : return 1\r\n",
    "\r\n",
    "    return 0\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "source": [
    "def matrix_profanity_check_overall(url, title, description, keyword, rating):\r\n",
    "    v_profanity_check_url = matrix_profanity_check_url(url)\r\n",
    "    v_profanity_check_title = matrix_profanity_check_text(title)\r\n",
    "    v_profanity_check_description = matrix_profanity_check_text(description)\r\n",
    "    v_profanity_check_keyword = matrix_profanity_check_text(keyword)\r\n",
    "    v_profanity_check_rating = matrix_profanity_check_rating(rating)\r\n",
    "\r\n",
    "    return {\"url\" : v_profanity_check_url , \"title\" : v_profanity_check_title ,\r\n",
    "     \"description\" : v_profanity_check_description , \"keyword\" : v_profanity_check_keyword ,\r\n",
    "      \"rating\" : v_profanity_check_rating }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "source": [
    "# url1 = \r\n",
    "# title1 = \r\n",
    "# desc1 =\r\n",
    "# rating =\r\n",
    "# keyword =\r\n",
    "\r\n",
    "# url1 = \"https://stackoverflow.com/questions/15367760/unboundlocalerror-local-variable-referenced-before-assignment-after-an-if-stahttps://stackoverflow.com/questions/15367760/unboundlocalerror-local-variable-referenced-before-assignment-after-an-if-sta\"\r\n",
    "# title1 = 'python - \"UnboundLocalError: local variable referenced before assignment\" after an if statement - Stack Overflow'\r\n",
    "# desc1 = \"\"\"I have also tried searching for the answer but I don't understand the answers to other people's similar problems...\r\n",
    "\r\n",
    "# tfile= open(\"/home/path/to/file\",'r') \r\n",
    "\r\n",
    "# def temp_sky(lreq, breq):\r\n",
    "#     for line in \"\"\"\r\n",
    "\r\n",
    "title1 = \"Free Porn, Sex, Tube Videos, XXX Pics, Pussy in Porno Movies - XNXX.COM\"\r\n",
    "url1 = \"https://www.xnxx.com/\"\r\n",
    "desc1 = \"XNXX delivers free sex movies and fast free porn videos (tube porn). Now 10 million+ sex vids available for free! Featuring hot pussy, sexy girls in xxx rated porn clips.\"\r\n",
    "rating1 = \"\"\r\n",
    "keyword1 = \"\"\r\n",
    "\r\n",
    "# profanity_check_overall(\"pornhub.com\",\"pornhub\",\"Pornhub is the world’s leading free porn site. Choose from millions of hardcore videos that stream quickly and in high quality, including amazing VR Porn. The largest adult site on the Internet just keeps getting better. We have more pornstars and real amateurs than anyone else. It’s fast, it’s free, it’s Pornhub!\")\r\n",
    "matrix_profanity_check_overall(url1,title1,desc1,rating1,keyword1)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'url': {'fword': 0, 'provlib': 0, 'ipdomain': 0},\n",
       " 'title': {'fword': 1, 'provlib': 1},\n",
       " 'description': {'fword': 1, 'provlib': 1},\n",
       " 'keyword': {'fword': 0, 'provlib': 0},\n",
       " 'rating': {'rating': 0}}"
      ]
     },
     "metadata": {},
     "execution_count": 135
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "559e3a93af9e93edb267577e6a3b4334b6a43fae2d008de4ecd8efbd96416806"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}